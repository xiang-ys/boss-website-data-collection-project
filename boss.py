import requests
import time
import random
import http.cookiejar
from urllib.parse import urljoin, urlencode, quote, urlparse
import json
from bs4 import BeautifulSoup
import pandas as pd
import re

# --- é…ç½® ---
COOKIE_FILE = 'cookies.txt'
SEARCH_KEYWORDS_LIST = ['']
TARGET_CITIES = {
    "æˆéƒ½": "101270100"
}
EXPERIENCE_CODE = '108'  #
MAX_PAGES_TO_SCRAPE_PER_CITY_KEYWORD = 10  # å…ˆåªçˆ¬ç¬¬ä¸€é¡µæµ‹è¯•
BASE_URL = 'https://www.zhipin.com'
DETAIL_PAGE_DELAY = 2
LIST_PAGE_DELAY_MIN = 2
LIST_PAGE_DELAY_MAX = 3
INTER_KEYWORD_DELAY_MIN = 2
INTER_KEYWORD_DELAY_MAX = 3

# API ç«¯ç‚¹
SET_TOKEN_URL = f"{BASE_URL}/wapi/zppassport/set/zpToken"
JOB_LIST_API_URL = f"{BASE_URL}/wapi/zpgeek/search/joblist.json"
JOB_DETAIL_API_URL = f"{BASE_URL}/wapi/zpgeek/job/detail.json"
MAIN_SEARCH_PAGE_URL = f"{BASE_URL}/web/geek/jobs"

# MYSTERIOUS_TOKEN = "IGJnYvfbmv2qeMfn" # æš‚æ—¶ç§»é™¤

# è¯·æ±‚å¤´
BASE_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br, zstd',
    'Connection': 'keep-alive',
    'sec-ch-ua': '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'Sec-Fetch-Site': 'same-origin',
}

# å®šä¹‰éœ€è¦æ›´æ–°Cookieçš„é”™è¯¯ç 
COOKIE_REFRESH_ERROR_CODES = [17, 37]
MAX_COOKIE_UPDATE_RETRIES_PER_CALL = 1  # æ¯ä¸ªAPIè°ƒç”¨ç‚¹å…è®¸çš„æœ€å¤§Cookieæ›´æ–°é‡è¯•æ¬¡æ•°


def load_cookies_from_file(session, cookie_file_path):
    print(f"â„¹ï¸ å°è¯•ä» '{cookie_file_path}' åŠ è½½Cookies...")
    cj = http.cookiejar.MozillaCookieJar(cookie_file_path)
    try:
        cj.load(ignore_discard=True, ignore_expires=True)
        # æ¸…ç©ºæ—§çš„ session cookies, å†åŠ è½½æ–°çš„
        session.cookies.clear()
        session.cookies.update(cj)
        print(f"ğŸª æˆåŠŸåŠ è½½/æ›´æ–°äº† '{cookie_file_path}' ä¸­çš„Cookiesã€‚")
        return True
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šCookieæ–‡ä»¶ '{cookie_file_path}' æœªæ‰¾åˆ°ã€‚")
        return False
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šä» '{cookie_file_path}' åŠ è½½Cookieå¤±è´¥: {e}")
        return False


def prompt_for_cookie_update(session):
    print("\n" + "=" * 30)
    print("â€¼ï¸ æ£€æµ‹åˆ°å¯èƒ½éœ€è¦æ›´æ–°Cookieï¼â€¼ï¸")
    print(f"è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š")
    print(f"  1. åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­ï¼Œè®¿é—® https://www.zhipin.com å¹¶è¿›è¡Œä¸€æ¬¡æˆåŠŸçš„æœç´¢ã€‚")
    print(f"  2. ä»æµè§ˆå™¨ä¸­å¯¼å‡ºæœ€æ–°çš„Cookieåˆ°åä¸º '{COOKIE_FILE}' çš„æ–‡æœ¬æ–‡ä»¶ä¸­ï¼Œå¹¶ç¡®ä¿å®ƒä¸æ­¤è„šæœ¬åœ¨åŒä¸€ç›®å½•ã€‚")
    print(f"  3. è¦†ç›–æ—§çš„ '{COOKIE_FILE}' æ–‡ä»¶ã€‚")
    print("=" * 30)

    while True:
        user_input = input(f"å®Œæˆä¸Šè¿°æ“ä½œåï¼Œè¯·è¾“å…¥ 'y' æˆ– 'yes' ä»¥ç»§ç»­ï¼Œæˆ–è¾“å…¥ 'n' æˆ– 'no' é€€å‡ºè„šæœ¬: ").strip().lower()
        if user_input in ['y', 'yes']:
            if load_cookies_from_file(session, COOKIE_FILE):
                print("âœ… Cookieå·²é‡æ–°åŠ è½½ï¼Œå°è¯•ç»§ç»­æ‰§è¡Œ...")
                # å¯é€‰ï¼šé‡æ–°è°ƒç”¨setToken APIï¼ˆå¦‚æœå®ƒæœ‰åŠ©äºæ¿€æ´»æ–°Cookieï¼‰
                # call_set_token_api(session)
                return True
            else:
                print("âŒ Cookieé‡æ–°åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æˆ–å†æ¬¡å°è¯•ã€‚")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œå¯ä»¥é€‰æ‹©å†æ¬¡æç¤ºæˆ–é€€å‡º
                # For simplicity, we'll let the loop ask again or user can choose to exit
        elif user_input in ['n', 'no']:
            print("ğŸ›‘ ç”¨æˆ·é€‰æ‹©é€€å‡ºè„šæœ¬ã€‚")
            return False
        else:
            print("âš ï¸ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ 'y'/'yes' æˆ– 'n'/'no'ã€‚")


def get_bst_token(session):
    for cookie in session.cookies:
        if cookie.name == 'bst' and cookie.domain == '.zhipin.com':
            return cookie.value
    return None


def fetch_job_list_page_html(session, page_num, city_code, query, experience_code_param,
                             current_lid=None, current_security_id=None,
                             retries_left=MAX_COOKIE_UPDATE_RETRIES_PER_CALL):
    bst_token_value = get_bst_token(session)
    params = {'scene': 1, 'query': query, 'city': city_code, 'page': page_num, 'pageSize': 30,
              '_': int(time.time() * 1000)}
    if experience_code_param: params['experience'] = experience_code_param
    if current_lid: params['lid'] = current_lid
    if current_security_id: params['securityId'] = current_security_id

    headers = BASE_HEADERS.copy()
    referer_params = {'query': query, 'city': city_code}
    if experience_code_param: referer_params['experience'] = experience_code_param
    referer_url = f"{MAIN_SEARCH_PAGE_URL}?{urlencode(referer_params)}"
    headers.update({
        'Accept': 'application/json, text/plain, */*', 'Referer': referer_url,
        'X-Requested-With': 'XMLHttpRequest', 'Sec-Fetch-Dest': 'empty', 'Sec-Fetch-Mode': 'cors',
    })
    if bst_token_value: headers['zp_token'] = bst_token_value

    try:
        print(f"ğŸ“„ æ­£åœ¨è·å– '{query}' èŒä½åˆ—è¡¨ç¬¬ {page_num} é¡µ (åŸå¸‚: {city_code})")
        response = session.get(JOB_LIST_API_URL, headers=headers, params=params, timeout=30)
        response.raise_for_status()  # å¯¹äºé2xxçŠ¶æ€ç ä¼šæŠ›å‡ºHTTPError

        response_json = response.json()
        api_code = response_json.get("code")

        if api_code in COOKIE_REFRESH_ERROR_CODES and retries_left > 0:
            print(f"  åˆ—è¡¨APIè¿”å›é”™è¯¯ç  {api_code} (æ¶ˆæ¯: {response_json.get('message')})ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieã€‚")
            if prompt_for_cookie_update(session):
                # Cookieæ›´æ–°æˆåŠŸï¼Œé‡è¯•ä¸€æ¬¡å½“å‰è¯·æ±‚
                print(f"  é‡è¯•è·å–åˆ—è¡¨API (å‰©ä½™é‡è¯•æ¬¡æ•°: {retries_left - 1})")
                return fetch_job_list_page_html(session, page_num, city_code, query, experience_code_param,
                                                current_lid, current_security_id, retries_left - 1)
            else:  # ç”¨æˆ·é€‰æ‹©ä¸æ›´æ–°æˆ–æ›´æ–°å¤±è´¥
                print("  ç”¨æˆ·æœªæ›´æ–°Cookieæˆ–æ›´æ–°å¤±è´¥ï¼Œåˆ—è¡¨APIè¯·æ±‚å¤±è´¥ã€‚")
                return response_json  # è¿”å›åŒ…å«é”™è¯¯ç çš„jsonï¼Œè®©ä¸Šå±‚å¤„ç†
        return response_json  # è¿”å›æ­£å¸¸çš„æˆ–æœ€ç»ˆçš„é”™è¯¯json

    except requests.exceptions.HTTPError as e_http:
        print(f"âŒ HTTPé”™è¯¯ - è¯·æ±‚åˆ—è¡¨API ({query}, é¡µ {page_num}): {e_http}")
        if e_http.response is not None:
            print(f"  å“åº”çŠ¶æ€: {e_http.response.status_code}, å“åº”å†…å®¹: {e_http.response.text[:300]}")
            # æŸäº›HTTPé”™è¯¯ä¹Ÿå¯èƒ½æ„å‘³ç€Cookieé—®é¢˜ï¼Œä½†è¿™é‡Œä¸»è¦é€šè¿‡APIçš„codeåˆ¤æ–­
            try:
                error_json = e_http.response.json()
                if error_json.get("code") in COOKIE_REFRESH_ERROR_CODES and retries_left > 0:
                    print(f"  HTTPé”™è¯¯ä¸­æ£€æµ‹åˆ°APIé”™è¯¯ç  {error_json.get('code')}ï¼Œå°è¯•Cookieæ›´æ–°æµç¨‹ã€‚")
                    if prompt_for_cookie_update(session):
                        return fetch_job_list_page_html(session, page_num, city_code, query, experience_code_param,
                                                        current_lid, current_security_id, retries_left - 1)
                    else:
                        return None  # è¿”å›Noneè¡¨ç¤ºè¯·æ±‚å½»åº•å¤±è´¥
            except ValueError:  # Not a JSON response
                pass
    except Exception as e:
        print(f"âŒ è¯·æ±‚åˆ—è¡¨APIæ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯ ({query}, é¡µ {page_num}): {e}");
    return None  # è¡¨ç¤ºè¯·æ±‚å¤±è´¥


def fetch_job_detail_api(session, security_id_param, lid_param, current_query, current_city_code,
                         retries_left=MAX_COOKIE_UPDATE_RETRIES_PER_CALL):
    if not security_id_param or not lid_param: return "å‚æ•°æ— æ•ˆ (security_id æˆ– lid_param ç¼ºå¤±)"

    api_call_delay = random.uniform(DETAIL_PAGE_DELAY, DETAIL_PAGE_DELAY + 5)  # ç¨å¾®ç¼©çŸ­éšæœºèŒƒå›´
    print(f"    API â³ å‡†å¤‡è·å–è¯¦æƒ…ï¼Œå»¶æ—¶ {api_call_delay:.2f} ç§’...")
    time.sleep(api_call_delay)

    bst_token_value = get_bst_token(session)
    params = {'securityId': security_id_param, 'lid': lid_param, '_': int(time.time() * 1000)}
    headers = BASE_HEADERS.copy()
    referer_params_detail = {'query': current_query, 'city': current_city_code}
    if EXPERIENCE_CODE: referer_params_detail['experience'] = EXPERIENCE_CODE
    referer_url_detail = f"{MAIN_SEARCH_PAGE_URL}?{urlencode(referer_params_detail)}"
    headers.update({
        'Accept': 'application/json, text/plain, */*', 'Referer': referer_url_detail,
        'X-Requested-With': 'XMLHttpRequest', 'Sec-Fetch-Dest': 'empty', 'Sec-Fetch-Mode': 'cors',
    })
    if bst_token_value: headers['zp_token'] = bst_token_value
    headers.pop('Sec-Fetch-User', None);
    headers.pop('Upgrade-Insecure-Requests', None)

    try:
        print(f"    API ğŸ” æ­£åœ¨è¯·æ±‚èŒä½è¯¦æƒ… (lid: {lid_param})")
        response = session.get(JOB_DETAIL_API_URL, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        detail_json = response.json()
        api_code = detail_json.get("code")

        if api_code in COOKIE_REFRESH_ERROR_CODES and retries_left > 0:
            print(f"  è¯¦æƒ…APIè¿”å›é”™è¯¯ç  {api_code} (æ¶ˆæ¯: {detail_json.get('message')})ï¼Œå¯èƒ½éœ€è¦æ›´æ–°Cookieã€‚")
            if prompt_for_cookie_update(session):
                print(f"  é‡è¯•è·å–è¯¦æƒ…API (å‰©ä½™é‡è¯•æ¬¡æ•°: {retries_left - 1})")
                # æ³¨æ„ï¼šå½“è¿”å›å­—ç¬¦ä¸²æ—¶ï¼Œè¡¨ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯JSONå¯¹è±¡
                return fetch_job_detail_api(session, security_id_param, lid_param, current_query, current_city_code,
                                            retries_left - 1)
            else:
                return f"è·å–æè¿°å¤±è´¥ (ç”¨æˆ·æœªæ›´æ–°Cookie, API code {api_code})"

        if api_code == 0:
            zp_data = detail_json.get("zpData", {});
            job_info = zp_data.get("jobInfo", {})
            description_from_api = job_info.get("postDescription")
            if description_from_api:
                soup_desc = BeautifulSoup(description_from_api, 'lxml')
                plain_description = soup_desc.get_text(separator='\n', strip=True)
                print(f"    API âœ… è¯¦æƒ…æå–æˆåŠŸ")
                return plain_description
            else:
                print(f"    API âš ï¸ JSONä¸­æ— æè¿° (zpData.jobInfo.postDescription)"); return "è·å–æè¿°å¤±è´¥ (JSONä¸­æ— æè¿°)"
        else:  # å…¶ä»–é0ä¸”éCookieåˆ·æ–°ç çš„é”™è¯¯
            print(f"    API âŒ è¯¦æƒ…APIè¿”å›é”™è¯¯: code {api_code}, msg: {detail_json.get('message')}")
            return f"è·å–æè¿°å¤±è´¥ (API code {api_code})"

    except requests.exceptions.HTTPError as e_http_detail:
        print(f"    API âŒ HTTPé”™è¯¯ - è¯·æ±‚è¯¦æƒ…API (lid: {lid_param}): {e_http_detail}")
        if e_http_detail.response is not None:
            print(
                f"      å“åº”çŠ¶æ€: {e_http_detail.response.status_code}, å“åº”å†…å®¹: {e_http_detail.response.text[:300]}")
            try:
                error_json = e_http_detail.response.json()
                if error_json.get("code") in COOKIE_REFRESH_ERROR_CODES and retries_left > 0:
                    print(f"  HTTPé”™è¯¯ä¸­æ£€æµ‹åˆ°APIé”™è¯¯ç  {error_json.get('code')}ï¼Œå°è¯•Cookieæ›´æ–°æµç¨‹ã€‚")
                    if prompt_for_cookie_update(session):
                        return fetch_job_detail_api(session, security_id_param, lid_param, current_query,
                                                    current_city_code, retries_left - 1)
                    else:
                        return "è·å–æè¿°å¤±è´¥ (ç”¨æˆ·æœªæ›´æ–°Cookie)"
            except ValueError:
                pass  # Not JSON
    except Exception as e:
        print(f"    API âŒ è¯·æ±‚è¯¦æƒ…APIæ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯ (lid: {lid_param}): {e}");
    return f"è·å–æè¿°å¤±è´¥ (è¯·æ±‚å¼‚å¸¸: {type(e).__name__})"


def parse_job_data_from_api_html(session, job_api_json_response, current_page_num, current_city_name, current_city_code,
                                 current_query):
    extracted_jobs = []
    if not job_api_json_response:
        print(f"âš ï¸ ä¼ å…¥çš„API JSONå“åº”ä¸ºç©º (åŸå¸‚ {current_city_name}, å…³é”®è¯ '{current_query}', é¡µ {current_page_num})")
        return extracted_jobs, False, None, None  # æ·»åŠ è¿”å›Falseè¡¨ç¤ºæ²¡æœ‰æ›´å¤š
    # API code æ£€æŸ¥å·²ç§»è‡³ fetch å‡½æ•°å†…éƒ¨ï¼Œè¿™é‡Œå‡è®¾ job_api_json_response æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„jsonï¼ˆå¯èƒ½æ˜¯é”™è¯¯ç jsonï¼‰
    if job_api_json_response.get("code") != 0:
        # å¦‚æœ fetch å‡½æ•°å¤„ç†äº† cookie æ›´æ–°å¹¶ä¸”ä»ç„¶å¤±è´¥ï¼Œå®ƒä¼šè¿”å›åŸå§‹çš„é”™è¯¯ json
        # æˆ‘ä»¬åœ¨è¿™é‡Œä¸éœ€è¦å†æ¬¡æç¤º cookie æ›´æ–°ï¼Œåªéœ€è®°å½•é”™è¯¯
        print(
            f"è§£æAPI: åˆ—è¡¨APIè¿”å›é”™è¯¯ç  (åŸå¸‚ {current_city_name}, å…³é”®è¯ '{current_query}', é¡µ {current_page_num}) - Code: {job_api_json_response.get('code')}, Message: {job_api_json_response.get('message')}")
        if job_api_json_response.get("code") in COOKIE_REFRESH_ERROR_CODES:
            print(f"  zpData (éªŒè¯ç ç›¸å…³?): {job_api_json_response.get('zpData')}")
        return extracted_jobs, False, None, None

    zp_data = job_api_json_response.get("zpData", {})
    if not zp_data: print(
        f"âš ï¸ APIå“åº”ä¸­æ—  'zpData' (åŸå¸‚ {current_city_name}, é¡µ {current_page_num})"); return extracted_jobs, False, None, None

    has_more = zp_data.get("hasMore", False)
    page_level_lid_for_next_list = zp_data.get("lid")
    page_level_security_id_for_next_list = zp_data.get("securityId")
    job_list_from_json = zp_data.get("jobList")

    if job_list_from_json and isinstance(job_list_from_json, list):
        print(f"â„¹ï¸ ç¬¬ {current_page_num} é¡µæ‰¾åˆ° {len(job_list_from_json)} ä¸ªåŸå§‹èŒä½æ¡ç›®ï¼Œå¼€å§‹è§£æ...")
        for job_item in job_list_from_json:
            # ... (å†…éƒ¨è§£æé€»è¾‘ä¸ä¹‹å‰ç‰ˆæœ¬ç±»ä¼¼ï¼Œç¡®ä¿è°ƒç”¨ fetch_job_detail_api æ—¶ä¼ é€’æ­£ç¡®çš„å‚æ•°) ...
            try:
                job_name = job_item.get("jobName", "N/A");
                salary_desc = job_item.get("salaryDesc", "N/A");
                brand_name = job_item.get("brandName", "N/A")
                location_name = job_item.get("locationName") or job_item.get("areaDistrict") or job_item.get(
                    "businessDistrict") or current_city_name
                display_location = f"{current_city_name} - {location_name}" if location_name != current_city_name else current_city_name
                experience = job_item.get("jobExperience", "N/A");
                education = job_item.get("jobDegree", "N/A");
                skills = job_item.get("skills", [])
                requirements_summary = " | ".join(filter(None, [experience, education] + skills))
                encrypt_job_id = job_item.get("encryptJobId");
                detail_api_security_id = job_item.get("securityId");
                detail_api_lid = job_item.get("lid")
                web_job_link = f"{BASE_URL}/job_detail/{encrypt_job_id}" if encrypt_job_id else "N/A"

                is_target_experience = False
                if EXPERIENCE_CODE is None or EXPERIENCE_CODE == '':
                    is_target_experience = True
                elif EXPERIENCE_CODE == '108':
                    if "å®ä¹ " in job_name or any(
                        "å®ä¹ " in str(req).lower() for req in [experience, education] + skills) or (
                            "ç»éªŒä¸é™" in experience and any(
                        term in job_name.lower() for term in ["å®ä¹ ", "intern"])): is_target_experience = True
                else:
                    if EXPERIENCE_CODE in requirements_summary: is_target_experience = True

                if is_target_experience:
                    job_data = {'æœç´¢å…³é”®è¯': current_query, 'åŸå¸‚': current_city_name, 'èŒä½åç§°': job_name,
                                'ç›´è¾¾é“¾æ¥': web_job_link, 'è–ªèµ„å¾…é‡': salary_desc, 'å…¬å¸åç§°': brand_name,
                                'å…·ä½“åœ°ç‚¹': display_location, 'è¦æ±‚æ ‡ç­¾': requirements_summary, 'èŒä½æè¿°': "å¾…è·å–"}
                    if detail_api_security_id and detail_api_lid:
                        description = fetch_job_detail_api(session, detail_api_security_id, detail_api_lid,
                                                           current_query, current_city_code)
                        job_data['èŒä½æè¿°'] = description
                    else:
                        job_data['èŒä½æè¿°'] = "ç¼ºå°‘å‚æ•°æ— æ³•è°ƒç”¨è¯¦æƒ…API"
                    extracted_jobs.append(job_data)
            except Exception as e:
                print(f"âŒ è§£æå•ä¸ªJSONèŒä½é¡¹æ—¶å‡ºé”™: {e}"); continue
        return extracted_jobs, has_more, page_level_lid_for_next_list, page_level_security_id_for_next_list
    else:
        print(f"â„¹ï¸ APIå“åº”çš„ zpData ä¸­æ—  'jobList' (åŸå¸‚ {current_city_name}, é¡µ {current_page_num})");
        return extracted_jobs, False, page_level_lid_for_next_list, page_level_security_id_for_next_list


def main():
    session = requests.Session()
    if not load_cookies_from_file(session, COOKIE_FILE):
        print("é¦–æ¬¡åŠ è½½Cookieå¤±è´¥ï¼Œè„šæœ¬æ— æ³•å¯åŠ¨ã€‚è¯·ç¡®ä¿cookies.txtå­˜åœ¨ä¸”æœ‰æ•ˆã€‚")
        return

    # call_set_token_api(session) # åˆå§‹setTokenå¯é€‰ï¼Œå¯ä»¥å…ˆä¸è°ƒç”¨

    all_jobs_across_all_searches = []
    keep_running = True  # æ§åˆ¶ä¸»å¾ªç¯æ˜¯å¦ç»§ç»­

    for search_query_keyword in SEARCH_KEYWORDS_LIST:
        if not keep_running: break  # å¦‚æœç”¨æˆ·é€‰æ‹©é€€å‡ºï¼Œåˆ™è·³å‡ºå¤–å±‚å¾ªç¯
        print(f"\n\nğŸ” --- å¼€å§‹çˆ¬å–å…³é”®è¯: '{search_query_keyword}' ---")
        if all_jobs_across_all_searches:
            delay = random.uniform(INTER_KEYWORD_DELAY_MIN, INTER_KEYWORD_DELAY_MAX)
            print(f"  ğŸ”„ åˆ‡æ¢å…³é”®è¯ï¼Œå»¶æ—¶ {delay:.2f} ç§’...")
            time.sleep(delay)

        for city_name_display, city_code_to_fetch in TARGET_CITIES.items():
            if not keep_running: break
            print(f"\n  ğŸ™ï¸ --- å¼€å§‹çˆ¬å–åŸå¸‚: {city_name_display} (ä»£ç : {city_code_to_fetch}) ---")

            current_search_jobs_list = []
            lid_for_next_list_page = None;
            security_id_for_next_list_page = None

            for page in range(1, MAX_PAGES_TO_SCRAPE_PER_CITY_KEYWORD + 1):
                if not keep_running: break
                print(f"\n  --- æ­£åœ¨å¤„ç† '{search_query_keyword}' - {city_name_display} - ç¬¬ {page} é¡µ ---")

                # è°ƒç”¨ fetch_job_list_page_htmlï¼Œå®ƒå†…éƒ¨åŒ…å«Cookieæ›´æ–°å’Œé‡è¯•é€»è¾‘
                job_page_api_response = fetch_job_list_page_html(
                    session, page, city_code_to_fetch, search_query_keyword, EXPERIENCE_CODE,
                    current_lid=lid_for_next_list_page, current_security_id=security_id_for_next_list_page)

                if not job_page_api_response:  # å¦‚æœå½»åº•å¤±è´¥ (ä¾‹å¦‚ç”¨æˆ·é€‰æ‹©ä¸æ›´æ–°Cookieå¹¶é€€å‡º)
                    print(f"  âŒ ç¬¬ {page} é¡µæ•°æ®è·å–å½»åº•å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¹²é¢„æˆ–Cookieå·²å¤±æ•ˆã€‚")
                    keep_running = False  # è®¾ç½®æ ‡å¿—åœæ­¢åç»­æ‰€æœ‰æ“ä½œ
                    break
                    # æ‰“å°ç¬¬ä¸€é¡µåˆ—è¡¨APIçš„å“åº”æ‘˜è¦
                if page == 1:
                    print(f"  --- ç¬¬1é¡µåˆ—è¡¨APIå“åº”ç : {job_page_api_response.get('code')} ---")
                    if job_page_api_response.get("code") != 0:
                        print(json.dumps(job_page_api_response, indent=2, ensure_ascii=False))
                    elif job_page_api_response.get("zpData"):
                        pass
                    print("  --- JSONå“åº”ç‰‡æ®µç»“æŸ ---\n")

                # åªæœ‰å½“åˆ—è¡¨APIè¯·æ±‚æœ€ç»ˆæˆåŠŸ (code == 0) æ—¶æ‰å»è§£æ
                if job_page_api_response.get("code") == 0:
                    parsed_jobs, has_more, next_lid_from_parse, next_security_id_from_parse = parse_job_data_from_api_html(
                        session, job_page_api_response, page, city_name_display, city_code_to_fetch,
                        search_query_keyword)

                    if not parsed_jobs and not has_more and job_page_api_response.get("zpData", {}).get(
                            "jobList") is not None and not job_page_api_response.get("zpData", {}).get("jobList"):
                        # APIæˆåŠŸè¿”å›ï¼ŒzpDataé‡Œæœ‰jobListä½†ä¸ºç©ºï¼Œä¸”hasMoreä¸ºfalse
                        print(f"  â„¹ï¸ ç¬¬ {page} é¡µAPIè¿”å›ç©ºèŒä½åˆ—è¡¨ä¸”æ— æ›´å¤šé¡µé¢ã€‚")
                    elif parsed_jobs:
                        current_search_jobs_list.extend(parsed_jobs)
                        print(f"  ğŸ‘ ç¬¬ {page} é¡µæˆåŠŸè§£æ {len(parsed_jobs)} ä¸ªç›®æ ‡èŒä½ã€‚")
                    else:
                        print(f"  â„¹ï¸ ç¬¬ {page} é¡µæœªè§£æåˆ°ç›®æ ‡èŒä½ã€‚")

                    lid_for_next_list_page = next_lid_from_parse
                    security_id_for_next_list_page = next_security_id_from_parse

                    if not has_more: print(f"  â„¹ï¸ APIæç¤ºç¬¬ {page} é¡µåæ²¡æœ‰æ›´å¤šã€‚"); break
                else:  # å¦‚æœåˆ—è¡¨APIæœ€ç»ˆè¿”å›é”™è¯¯
                    print(f"  âŒ åˆ—è¡¨APIæœ€ç»ˆè¿”å›é”™è¯¯ (Code: {job_page_api_response.get('code')})ï¼Œç»ˆæ­¢æ­¤å…³é”®è¯/åŸå¸‚ã€‚")
                    if job_page_api_response.get("code") in COOKIE_REFRESH_ERROR_CODES:
                        print("    è¿™é€šå¸¸æ„å‘³ç€Cookieå·²å¤±æ•ˆæˆ–éœ€è¦éªŒè¯ã€‚")
                    break

                if page < MAX_PAGES_TO_SCRAPE_PER_CITY_KEYWORD and has_more:
                    list_delay = random.uniform(LIST_PAGE_DELAY_MIN, LIST_PAGE_DELAY_MAX)
                    print(f"  ğŸ“„ å»¶æ—¶ {list_delay:.2f} ç§’åè·å–ä¸‹ä¸€åˆ—è¡¨é¡µ...")
                    time.sleep(list_delay)

            all_jobs_across_all_searches.extend(current_search_jobs_list)
            print(
                f"  âœ… --- å…³é”®è¯ '{search_query_keyword}', åŸå¸‚: {city_name_display} å¤„ç†å®Œæˆ, å…±æ‰¾åˆ° {len(current_search_jobs_list)} ä¸ªèŒä½ ---")
            if not keep_running: break
            # time.sleep(random.uniform(INTER_KEYWORD_DELAY_MIN / 2, INTER_KEYWORD_DELAY_MAX / 2))

    print(f"\nğŸ‰ğŸ‰ğŸ‰ --- æ‰€æœ‰çˆ¬å–å®Œæˆæˆ–ç”¨æˆ·é€‰æ‹©é€€å‡º --- ğŸ‰ğŸ‰ğŸ‰")
    # ... (åç»­çš„Excel/JSONè¾“å‡ºé€»è¾‘ä¸å˜) ...
    if all_jobs_across_all_searches:
        print(f"æ€»å…±æ‰¾åˆ° {len(all_jobs_across_all_searches)} ä¸ªç›¸å…³èŒä½ã€‚\n")
        df = pd.DataFrame(all_jobs_across_all_searches)
        excel_columns_order = ['æœç´¢å…³é”®è¯', 'åŸå¸‚', 'èŒä½åç§°', 'å…¬å¸åç§°', 'è–ªèµ„å¾…é‡', 'å…·ä½“åœ°ç‚¹', 'è¦æ±‚æ ‡ç­¾',
                               'èŒä½æè¿°', 'ç›´è¾¾é“¾æ¥']
        df_columns = [col for col in excel_columns_order if col in df.columns]
        df_to_save = df[df_columns]
        timestamp_str = time.strftime("%Y%m%d_%H%M%S")
        excel_output_filename = f"BOSSç›´è˜_å²—ä½_{'_'.join(TARGET_CITIES.keys())}_{timestamp_str}.xlsx"
        try:
            df_to_save.to_excel(excel_output_filename, index=False, engine='openpyxl')
            print(f"ğŸ’¾ ç»“æœå·²æˆåŠŸä¿å­˜è‡³ Excel æ–‡ä»¶: '{excel_output_filename}'")
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ° Excel æ–‡ä»¶å¤±è´¥: {e}")
            json_output_filename = f"BOSSç›´è˜_å²—ä½_{'_'.join(TARGET_CITIES.keys())}_{timestamp_str}.json"
            with open(json_output_filename, 'w', encoding='utf-8') as f_out:
                json.dump(all_jobs_across_all_searches, f_out, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ JSON æ–‡ä»¶: '{json_output_filename}'")
        print("\n--- æ§åˆ¶å°è¾“å‡ºç¤ºä¾‹ (æœ€å¤šå‰2æ¡) ---")
        for i, job in enumerate(all_jobs_across_all_searches[:2]):
            print(f"\nğŸ’¼ --- èŒä½ {i + 1} ({job.get('æœç´¢å…³é”®è¯', '')} - {job.get('åŸå¸‚', '')}) ---")
            for k, v_job in job.items():
                if k == 'èŒä½æè¿°' and isinstance(v_job, str):
                    print(f"  {k}: {v_job[:60].replace(chr(10), ' ')}...")
                else:
                    print(f"  {k}: {v_job}")
            print("-" * 40)
    else:
        print("âŒ æ‰€æœ‰åŸå¸‚å’Œå…³é”®è¯ç»„åˆå‡æœªèƒ½æ‰¾åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„èŒä½ï¼Œæˆ–è€…åœ¨è¿‡ç¨‹ä¸­é€€å‡ºäº†ã€‚")

if __name__ == '__main__':
    print("è„šæœ¬å¼€å§‹è¿è¡Œã€‚å½“æç¤ºæ›´æ–°Cookieæ—¶ï¼Œè¯·ç¡®ä¿ cookies.txt æ–‡ä»¶æ˜¯æœ€æ–°ä¸”æœ‰æ•ˆçš„ï¼")
    print("åœ¨æµè§ˆå™¨ä¸­å®Œæˆä¸€æ¬¡æˆåŠŸæœç´¢å¹¶ç«‹å³å¯¼å‡ºCookieï¼Œå¯ä»¥æé«˜æˆåŠŸç‡ã€‚")
    main()